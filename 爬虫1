from copyheaders import headers_raw_to_dict
from bs4 import BeautifulSoup
import requests
import time
import random

######### 爬虫 ##############
headers = b"""
accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
accept-encoding:gzip, deflate, br
accept-language:zh-CN,zh;q=0.9
cache-control:max-age=0
cookie:_T_WM=1ad1436888fe8b89c7a2863f7986d7b9; SUB=_2A25xpfVRDeRhGeRI6VMT8C3FzT-IHXVTaZsZrDV6PUJbkdAKLWqmkW1NUtUcliAU3-z50ac51gSoCPiLVoblnWq1; SUHB=0wn0NF3PkyCp1I; SCF=Anw_ufU9Bwimf_jyIpvjZPuSWOGgMSGxwWbMWbz6_lrc9w18Uz2_qdur67eB2PdHI7SHFiCLhZYDCtIgm7ehNcc.
upgrade-insecure-requests:1
user-agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36
"""

# 将请求头字符串转化为字典
headers = headers_raw_to_dict(headers)
# 评论的页数，有多少页，这里就写多少
page = 3000
for i in range(1, page):
    try:
        if i % 50 == 1:
            # 每隔50页打印一下日志
            print("正在抓取第{}页".format(i))
        time.sleep(random.choice(range(10)))
        # 请求网址， 替换网址， 进行爬虫
        url = 'https://weibo.cn/comment/HwokcrhjJ?uid=1784473157&rl=1&page=' + str(i)
        # 发送get请求
        response = requests.get(url=url, headers=headers)
        # 获取评论内容
        html = response.text
        # 利用BeautifulSoup解析网页
        soup = BeautifulSoup(html, 'html.parser')
        # 获取评论信息
        result_1 = soup.find_all(class_='ctt')
        try:
            # 遍历每一页的全部评论
            for j in range(len(result_1)):
                #读取评论信息
                text = result_1[j].get_text().replace(',', '，')
                # 写入csv
                comment = text.split(':')[-1]
                print(comment)
                with open('结果.txt', 'a+') as f:
                    f.writelines(comment + "\n")
        except:
            continue
    except:
        continue
